#+title: Posts
#+author:
#+options: ^:nil _:nil

#+hugo_base_dir: ~/Writing/githegipen
#+hugo_section: post
#+hugo_front_matter_format: yaml

#+macro: sidenote @@hugo:{{<sidenote>}}$1{{</sidenote>}}@@
#+macro: margin @@hugo:{{<sidenote>}}$1{{</sidenote>}}@@

* Quadratic Mutual Information (in brief)
:PROPERTIES:
:export_file_name: intro_to_qmi
:export_date: 2023-02-01
:export_hugo_custom_front_matter: :math true :draft true
:export_hugo_paired_shortcodes: sidenote
:END:
#+begin_comment
Outline
1. What is it? (Derivation)
2. Estimating it
3. Optimising it
4. Some uses (old and new)

Idea: two scatter map of different bivariate distributions (isotropic Gaussian, correlated gaussian, spherical), each with a different hue determinant. In the first hue is determined by the first term in Vc, and in the second by the second. This is supposed to show how high cross-info-potential relates to low MI. We expect the isometric Gaussian to have the highest "heat" values at each sample.

#+end_comment

Information theory is great -- in theory. In practice, many of the quantities are tricky to compute. Most of them revolve around entropy

\[
H(X) = -\sum_{i=1}^N \log p_j,
\]

which is fine for discrete systems for which we know all possible states /and/ the distribution over those states. Most often this is far from the case, so we have to be clever about how we use these measures.

Thankfully, the work of past clever people cascades down to the rest of us, and we can reap the efforts of their cleverness. In 1994, Jagat Narain Kapur published a treatise: /Measures of Information and their Application/ [1] in which he formulated alternative forms of mutual information with varying uses. Of note, to us, is what later developers of the theory called: Quadratic Mutual Information (QMI) [2]. The idea is that replacing Kullback-Liebler (KL) divergence in QMI with a 'Quadratic Divergence' results in a form that's /much/ easier to estimate in a differentiable way. The trade-off is that the raw QMI quantity isn't meaningful the same way MI is, but here's the kicker: the same distributions that maximise QMI also maximise MI. So we can optimise QMI, a differentiable and computationally-cheap-to-estimate quantity, as a proxy to MI. How great!

** Deriving QMI
Both Jose Principe's book /Information Theoretic Learning/ [2] and Kari Torkkola's paper [3] have well-expounded derivations of QMI and its estimator. Here I'll re-elaborate on what has already been done there.

Mutual information between two random variables \(X\) and \(Y\) can be thought of as the KL-divergence between their joint distribution, and the product of their marginals,
\[
I(X; Y) = D_{KL}(P_{X,Y}\|P_XP_Y) = \iint\log\frac{p(x,y)}{p(x)p(y)}\:dxdy.
\]

With QMI we replace KL-divergence and replace it with something that looks similar to Euclidean distance,

\[
I_Q(X; Y) = \iint(p(x,y) - p(x)p(y))^2\:dxdy.
\]

Expanding this we get a three-term quadratic-looking expression,

\begin{align*}
I_Q(X; Y) = \iint p(x,y)^2 - 2p(x,y)p(x)p(y) + p(x)^2p(y)^2.\tag{1}
\end{align*}

And so there it is, quadratic mutual information. Yet still not as useful in practice. We're still stuck with those gnarly integrals and density functions we have to somehow estimate. In comes the second step: the QMI estimator.

** The QMI estimator
First, we use [[https://www.wikiwand.com/en/Kernel_density_estimation][Kernel Density Estimation]] (KDE) to approximate the density functions. Given dataset of \(N\) samples \(\{(x_i,y_i)|i=1...N\}\), we estimate the densities using a Gaussian kernel with bandwidth \(\sigma\):

\begin{align*}
\hat{p}_{X,Y}(x, y) &= \frac{1}{N}\sum_{i=1}^N G_\sigma(x-x_i)G_\sigma(y-y_i)\tag{2}\\
\hat{p}_X(x) &= \frac{1}{N}\sum_{i=1}^NG_\sigma(x-x_i),\tag{3}\\
\hat{p}_Y(y) &= \frac{1}{N}\sum_{i=1}^NG_\sigma(y-y_i),\tag{4}
\end{align*}

where \(G_\sigma(x)\) is the Gaussian density function with \(\mu=0\) and \(\sigma\) standard deviation. Before we plug the KDE estimates into (1), we note the following property of convolved Gaussian functions:

\[
\int G_\sigma(x-x')G_\sigma(y-y')\:dx'dy' = G_{\sigma\sqrt{2}}(x' - y').
\]

This neat simplification /vanquishes/ the scary integrals of \((1)\){{{margin(This is why we use the Gaussian kernel in particular.)}}},
leaving us with pleasant sums-of-sums that we can compute exactly. In this manner, each of three terms in \((1)\) becomes an /information potential/ (IP): the joint-, marginal-, and cross-information potentials respectively,
\[
\hat{I}_Q(X; Y) = \hat{V}_J + \hat{V}_M - 2\hat{V}_C,\\
\]

where the IPs work out to:

\begin{align*}
\hat{V}_J &= \frac{1}{N^2}\sum_{i=1}^N\sum_{j=1}^N G_{\sigma\sqrt2}(x_i - x_j)G_{\sigma\sqrt2}(y_i - y_j)\tag{5}\\
\hat{V}_M &= \bigg(\frac{1}{N^2}\sum_{i=1}^N\sum_{j=1}^N G_{\sigma\sqrt2}(x_i-x_j)\bigg)\bigg(\frac{1}{N^2}\sum_{i=1}^N\sum_{j=1}^N G_{\sigma\sqrt2}(y_i-y_j)\bigg)\tag{6}\\
\hat{V}_C &= \frac{1}{N}\sum_{i=1}^N
\bigg( \frac{1}{N}\sum_{j=1}^N G_{\sigma\sqrt{2}}(x_i - x_j) \bigg)
\bigg( \frac{1}{N}\sum_{j=1}^N G_{\sigma\sqrt{2}}(y_i - y_j) \bigg).\tag{7}
\end{align*}

** Computation
If you squint a little you'll see that \((5)-(7)\) revolve around inter-sample differences \(x_i-x_j\) and \(y_i-y_j\). This means we can compute all of these /once/ and just re-use them for each IP, bringing the time complexity of QMI into the \(\mathcal{O}(N^2)\) range. Let \(X \in \mathbb{R}^{N\times N}\) be the /difference matrix/ for \(x\)-samples such that \(X_{ij} = G_{\sigma\sqrt{2}}(x_i - x_j\), and similarly for \(Y\). We can then rewrite the IPs  with Einstein notation:

\begin{align*}
\hat{V}_J &= N^{-2} X_{ij}Y_{ij}\\
\hat{V}_M &= N^{-4} (\mathbf{1}_{ij}X_{ij})(\mathbf{1}_{kl}Y_{kl}) \\
\hat{V}_C &= N^{-3} (\mathbf{1}_jX_{ij})(\mathbf{1}_kY_{ik}),
\end{align*}

where summation over the indices is implied and \(\mathbf{1}\) is the vector/matrix of all ones (either/or implied by the number of indices); \(\mathbf{1}_{ij}A_{ij}\) is shorthand for summing over all the elements in \(A_{ij}\). The notation might be a little obtuse at first but it serves to show how simple the calculations really are.

The above translates to Python quite smoothly:
#+begin_src python
def qmi(x, y):
    """Compute QMI
    Args:
        x and y: Length N ndarrays (samples from random variables X and Y)
    """
    # Compute difference matrix (G is the elementwise univariate Gaussian pdf)
    X = G( x[:,None] - x )
    Y = G( y[:,None] - y )

    # Compute information potentials
    Vj = (X * Y).mean()
    Vm = X.mean() * Y.mean()
    Vc = (X.mean(axis=1) * Y.mean(axis=1)).mean()

    # QMI
    return Vj + Vm - 2*Vc
#+end_src


** Applications
Here we've gone over the QMI formulation where both \(x\) and \(y\) are continuous-valued. This form is the most general and, fortunately, the most straightforward. In his 2002 paper, Torkkola [3] formulated QMI for the discrete-continuous case. There, one variable was class labels and the other a continuous (possibly multivariate) value. In that context QMI becomes a way to do feature selection: maximising the QMI between a transformation \(z = f_\theta(x)\) and the class labels, w.r.t. parameters \(\theta\), puts \(z\) in a space where same-class samples cluster and differing-class ones are far apart -- effectively, classification.

QMI has been used this way in neuroscience [4,5]. The binary class labels become the presence/absence of a spike in a single neuron, \(x\) is whichever stimulus was used, and the transformation (often linear) projects the stimulus into a space where its samples are separated by whether they evoked a spike or not. Thus, the parameters of the transformation are the neuron's receptive field: the stimulus that it's sensitive to.

Though useful so far, QMI, as far as I can tell, seems to be relatively untapped. I mean, this is an easy way to optimise mutual information, we could use it in loads of places{{{margin(Caveat: the particular way QMI is optimised is important\, particularly how bandwidth $\sigma$ is adjusted. This is worth talking about in a future article.)}}}. For instance, we could apply the same receptive-field mapping procedure to neurons/layers in a deep neural net and do something like [[https://distill.pub/2017/feature-visualization/][feature visualisation]]. It would be great to see this used more.


** References
[1] Kapur, J.N. (1994). /Measures of information and their applications./

[2] Xu, D., Erdogmuns, D. (2010). /Renyiâ€™s Entropy, Divergence and Their Nonparametric Estimators. In: Information Theoretic Learning. Information Science and Statistics./

[3] Torkkola, K. (2002). /On feature extraction by mutual information maximization./

[4] Mu, Z., Nikolic, K., Schultz, S.R. (2021). /Quadratic Mutual Information estimation of mouse dLGN receptive fields reveals asymmetry between ON and OFF visual pathways./

[5] Katz, M., Viney, T., Nikolic, K. (2016). /Receptive Field Vectors of Genetically-Identified Retinal Ganglion Cells Reveal Cell-Type-Dependent Visual Functions./

* Thinking with subspaces
:PROPERTIES:
:export_file_name: thinking_with_subspaces
:export_date: 2025-02-16
:export_hugo_custom_front_matter: :math true :draft true
:export_hugo_paired_shortcodes: sidenote
:END:
In the first chapter of the book [[https://www.google.co.uk/books/edition/From_Vector_Spaces_to_Function_Spaces/hXt_wSTNhb0C?hl=en][From Vector Spaces to Function Spaces]] its
author Yutaka Yamamoto makes this statement (on the reason we generalise the
usual intuitive notion of a "vector" as a list of numbers to something more abstract):

#+BEGIN_QUOTE
... such a situation is entirely standard in modern mathematics, especially in
algebra. Since we deal more with operations and structures induced from them,
the peculiarity of an individual element becomes, inevitably, less significant
than the totality that constitutes the entire structure.
#+END_QUOTE
The phrase "operations and structures induced from them" opened a door in my
mind.

When I thought of math, I would think of the process of beginning from axioms or
desirable properties and discovering the ensuing tree of logical
relationships; /understanding/ in this case thus meant knowing what implies what,
and under what conditions.

But this phrase helped me zoom out and see learning math as understanding the
/shape/ of this logical structure; looking for recurring patterns in this shape.
This is the difference between knowing how to climb a tree and being able to
paint a pretty good picture of it. You've got to do both---in fact the two are
synergistic.

But today I want to do more than climb. I want to look at the first few, simple
branches of a vast and complex tree, many of whose leaves I still glance up
humbly at and hope to climb: the tree of Linear Algebra. Or, more precisely, of
finite-dimensional vector spaces. A tree whose base you can scale through the
early chapters of any good textbook on the subject; and which, in climbing,
you'll encounter the fundamental notion we're concerned with today: that of a
_subspace_.

My objective here is to not just understand what a subspace is, but to see the
forest for the trees, so to speak---to arrive at a more intuitive pattern for
understanding just what the heck this linear algebra business is about.

** Span
Let's start with a simple object \(v\). We'll call this object a vector, but
only for reasons which will become clear later. For now the object has no
properties, and is necessarily nothing. It's not a list of numbers, it's not a
function, it's not a sequence---it could potentially be any of these things but
what it is doesn't matter for our purposes. We begin with a totally abstract
object \(v\) and go from there.

Let's pick a [[https://en.wikipedia.org/wiki/Field_(mathematics)][field]] \(F\), and define another vector \(u = \alpha v\), where \(\alpha\in F\).
In fact let's define a whole set of such objects \(S_v = \{ \alpha v: \alpha\in F \}\}\) for
every field member. We call this set the "span" of \(v\).

[WE NEED TO HAVE DISTRIBUTIVITY TO PROCEED]

Now we notice the following two interesting properties.

First, \(S_v\) contains all scalar multiplications[fn::I know we haven't
properly defined scalar multplication but I will] of itself. This is trivial,
since if we consider any \(u=\alpha v\) we see that \(\beta u =(\beta\alpha)v\inS_v\), since
\(S_v\) contains all scalar multiples of \(v\).

Second, any weighted sum of two
elements of \(S_v\) are contained in \(v\). For instance, let \(u_1=\alpha_1 v\) and
\(u_2=\alpha_2v\). We see that \(u_1+u_2=(\alpha_1+\alpha_2)v\in S_v\). In general, any finite
sum of elements \(u_k=\alpha_kv\) for \(k=1,...,n\) is in \(S_v\) since
\[
u_1+u_2+\vdots+u_n = (\alpha_1+\alpha_2+\vdots+\alpha_n)v.
\]

So we have this special set defined w/r/t an object \(v\) that contains all
scalar multiples of \(v\) and, as we've shown, necessarily contains all sums and
scalar multiples of any of its elements.

[WE@VE UTILISED VECTOR SPACE PROPERTIES HERE WITHOUT DEFINING THEM LIKE DISTRIBUTIVITY]

** Combining Spans
Now say we have a second object \(w\). Like \(v\), we can create its span
\(S_w\). The sets \(S_v\) and \(S_w\) are totally disjoint, since we don't have
any meaningful notion of equality between \(v\) and \(w\); they're totally
abstract objects.

But, for no reason but pure curiosity, we can ask: can we "combine" these spaces
somehow? Let's /define/ an element \(v+w\). To be clear, this is not the
"addition" of \(v\) and \(w\) in any meaningful sense, like addition between
reals or rationals is meaningful. The symbol \(+\) is used to suggest the
relationship is /similar/ to a sum, but we don't know much about this relationship
quite yet, since we're in the process of defining it.

Let's call \(v+w\) a vector, same as \(v\) and \(w\).
